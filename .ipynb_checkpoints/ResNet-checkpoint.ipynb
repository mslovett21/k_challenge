{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_fingerprints_fh = 'sample/sample_fingerprints.csv'\n",
    "drug_targets_fh      = 'sample/sample_targets.csv'\n",
    "drug_weights_fh      = 'sample/sample_weights.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size       = 10000\n",
    "fingerprint_size  = 1024\n",
    "fingerprint_width = 32\n",
    "targets_num       = 420\n",
    "weights_num       = 420\n",
    "num_channels      = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def populate_data(file_handle,data_matrix, data_size):\n",
    "    with open(file_handle) as fh:\n",
    "        j=0\n",
    "        content = fh.readlines()\n",
    "        content = [x.strip() for x in content]\n",
    "        for line in content:\n",
    "            result = re.split(r'[,\\t]\\s*',line)\n",
    "            for i in range(1,data_size+1):\n",
    "                data_matrix[j][i-1] = np.float32(result[i])\n",
    "            j = j+1\n",
    "    print(j)\n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_fingerprints = []\n",
    "drug_targets      = []\n",
    "drug_weights      = []\n",
    "\n",
    "\n",
    "for i in range(sample_size):\n",
    "    fingerprint_holder = [0]* fingerprint_size\n",
    "    drug_fingerprints.append(fingerprint_holder)\n",
    "    \n",
    "for i in range(sample_size):\n",
    "    target_holder = [0]* targets_num\n",
    "    drug_targets.append(target_holder)\n",
    "\n",
    "for i in range(sample_size):\n",
    "    weight_holder = [0]* weights_num\n",
    "    drug_weights.append(weight_holder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populate_data(drug_weights_fh, drug_weights, weights_num)\n",
    "populate_data(drug_targets_fh, drug_targets, targets_num)\n",
    "populate_data(drug_fingerprints_fh, drug_fingerprints, fingerprint_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_fingerprints = np.array(drug_fingerprints)\n",
    "drug_targets      = np.array(drug_targets)\n",
    "drug_weights      = np.array(drug_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/mjdietzx/0cb95922aac14d446a6530f87b3a04ce#file-residual_network-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tensorflow/tpu/blob/master/models/official/resnet/resnet_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **TensorFlow** graph consists of the following parts:\n",
    "\n",
    "* **Placeholder** variables used for inputting data to the graph.\n",
    "* **Variables** that are going to be optimized so as to make the convolutional network perform better.\n",
    "* The mathematical formulas for the convolutional network.\n",
    "* **Cost function** be used to guide the optimization of the variables.\n",
    "* **Optimization** method which updates the variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, fingerprint_size],name = \"In_Flat_Drug_Fingerprint\")\n",
    "\n",
    "drug_image = tf.reshape(x, [-1, fingerprint_width, fingerprint_width, num_channels], name=\"Drug_Image_32x32\")\n",
    "\n",
    "y_true = tf.placeholder(tf.float32, [None, targets_num],name='True_Labels')\n",
    "\n",
    "cross_entropy_weights = tf.placeholder(tf.float32, [None, weights_num],name = \"Cross_Entropy_Weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05), name=\"Weights\")\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]), name=\"Biases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NETWORK ARCHITECTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/ResNetPic.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Description\n",
    "*  **conv1** (7x7 conv, 64,/2) -> ***filter_size***=7, ***out_channels***=64, ***stride***=2\n",
    "*  **pooling layer** ***stride***=2\n",
    "*  **block1** layers 6x[(3x3 con,64,)] -> 6 conv layers with:    ***filter_size***=3, ***out_channels***=64, ***stride***=1\n",
    "*  **block2** layers 8x[(3x3 con,128,)] -> 8 conv layers with:   ***filter_size***=3, ***out_channels***=128, ***stride***=1\n",
    "*  **block3** layers 12x[(3x3 con,256,)] -> 12 conv layers with: ***filter_size***=3, ***out_channels***=265, ***stride***=1 \n",
    "*  **block4** layers 6x[(3x3 con,512,)] -> 6 conv layers with:   ***filter_size***=3, ***out_channels***=512, ***stride***=1\n",
    "* **average pooling**\n",
    "* **fully connected layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difficulty: Shortcut Connection (FIRST OROGINAL IMPLEMENTATION)\n",
    "The identity shortcuts can be directly used when the input and output are of the same dimensions (solid line shortcuts on the graph above). When the dimension increase, we have two options:\n",
    "<br>\n",
    "* (A) the shortcut still performs identity mapping with extra zero passed for increasing dimensions,\n",
    "* (B) the shortcut is used to match dimensions (done by 1x1 convolution)\n",
    "<br>\n",
    "<br>\n",
    "For both options, when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/better_residue.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/two_res_unit.png\" alt=\"Drawing2\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info about Convolution layer API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.nn.conv2d` function from TensorFlow API\n",
    "<br>\n",
    "<br>\n",
    "`tf.nn.conv2d`(\n",
    "<br>\n",
    "    input,\n",
    " <br>\n",
    "    filter,\n",
    "    <br>\n",
    "    strides,\n",
    "    <br>\n",
    "    padding,\n",
    "    <br>\n",
    "    use_cudnn_on_gpu=True,\n",
    "    <br>\n",
    "    data_format='NHWC',\n",
    "    <br>\n",
    "    dilations=[1, 1, 1, 1],\n",
    "    <br>\n",
    "    name=None\n",
    ")\n",
    "<br>\n",
    "<br>\n",
    "Computes a 2-D convolution given 4-D input and filter tensors.\n",
    "<br>\n",
    "<br>\n",
    "Given an input tensor of shape `[batch, in_height, in_width, in_channels]` and a filter / kernel tensor of shape `[filter_height, filter_width, in_channels, out_channels]`, this op performs the following:\n",
    "<br>\n",
    "<br>\n",
    "* Flattens the filter to a 2-D matrix with shape `[filter_height * filter_width * in_channels, output_channels]`.\n",
    "* Extracts image patches from the input tensor to form a virtual tensor of shape `[batch, out_height, out_width, filter_height * filter_width * in_channels]`.\n",
    "* For each patch, right-multiplies the filter matrix and the image patch vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_conv_layer(input,              # previous layer in the network\n",
    "                   filter_size,        # size of a filer, width=height=size\n",
    "                   in_channels,        # number of channels in the input layer\n",
    "                   out_channels,       # number of channels in the output layer, aka number of filers\n",
    "                   stride = 1):        # strides = [1,stride, stride, 1]\n",
    "    \n",
    "    #Shape of the filter weights for convolution\n",
    "    shape = [filter_size, filter_size, in_channels, out_channels]\n",
    "    \n",
    "    #Create new weights = new filters of specified dimensions\n",
    "    weights = new_weights(shape = shape)\n",
    "    \n",
    "    #Create new biases, one for each filter\n",
    "    biases = new_biases(length = out_channels)\n",
    "    \n",
    "    #Create a new TensorFlow operation for convolution.\n",
    "    layer = tf.nn.conv2d(input = input, filter = weights, strides = [1, stride, stride, 1],\n",
    "                         padding=('SAME' if stride == 1 else 'VALID'), name = \"CONV\")\n",
    "    \n",
    "    layer = layer + biases\n",
    "    \n",
    "    return layer   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info about Max Pooling layer API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.nn.max_pool`(\n",
    "<br>\n",
    "    value,\n",
    "    <br>\n",
    "    ksize,\n",
    "    <br>\n",
    "    strides,\n",
    "    <br>\n",
    "    padding,\n",
    "    <br>\n",
    "    data_format='NHWC',\n",
    "    <br>\n",
    "    name=None\n",
    ")\n",
    "<br>\n",
    "* `value`: A 4-D Tensor of the format specified by data_format.\n",
    "* `ksize`: A list or tuple of 4 ints. The size of the window for each dimension of the input tensor.\n",
    "* `strides`: A list or tuple of 4 ints. The stride of the sliding window for each dimension of the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_pooling_layer(input,dim_ksize, stride):\n",
    "    \n",
    "    layer = tf.nn.max_pool(value=input,ksize=[1, dim_ksize, dim_ksize, 1],strides=[1, stride, stride, 1],\n",
    "                           padding='SAME', name = \"POOLING_LAYER\")\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_NORM_DECAY = 0.9\n",
    "BATCH_NORM_EPSILON = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm_and_RELU(inputs,axis=3, relu = True):\n",
    "    \n",
    "    # axis= 3 because input's format is [num_images, img_height, img_width, num_channels]   \n",
    "    #layer = tf.layers.batch_normalization(inputs=inputs,axis=axis,center=True,\n",
    "     #                                     scale=True,training=True, fused=True)\n",
    "    layer = tf.layers.batch_normalization(inputs=inputs,axis=axis)\n",
    "    if relu:\n",
    "        layer = tf.nn.relu(layer) \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORIGINAL IMPLEMENTATION OF THE RESIDUAL UNIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_unit(inputs,filter_size,in_channels,out_channels, strides,use_projection=False):\n",
    "    \n",
    "    shortcut = inputs\n",
    "    print(\"Residue unit:\")\n",
    "    \n",
    "    if use_projection:\n",
    "        kernel = 1\n",
    "        shortcut = new_conv_layer(input=inputs,filter_size = kernel,in_channels= in_channels,\n",
    "                                  out_channels = out_channels,stride = 2)\n",
    "        print(\"FIRST IN THE BLOCK\")\n",
    "        shortcut = batch_norm_and_RELU(shortcut, relu = False)\n",
    "    print(\"Shortcut:\")\n",
    "    print(shortcut)\n",
    "\n",
    "    print(\"CONV 1\")\n",
    "    inputs = new_conv_layer(input=inputs,filter_size = filter_size,in_channels = in_channels,\n",
    "                            out_channels = out_channels,stride = 1)\n",
    "    print(inputs)\n",
    "    print(\"\\n\")\n",
    "    inputs = batch_norm_and_RELU(inputs, relu = True)\n",
    "    \n",
    "    print(\"CONV 2\")\n",
    "\n",
    "    inputs = new_conv_layer(input=inputs,filter_size = filter_size,in_channels = in_channels,\n",
    "                            out_channels = out_channels,stride = 1)\n",
    "    print(inputs)\n",
    "    print(\"\\n\") \n",
    "    inputs = batch_norm_and_RELU(inputs, relu = False)\n",
    "    \n",
    "    print(\"Added Shortcut: \")\n",
    "    sum_with_shortcut = inputs + shortcut\n",
    "    print(sum_with_shortcut)\n",
    "    print(\"\\n\")\n",
    "    return tf.nn.relu(sum_with_shortcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_group(inputs, blocks,filter_size, in_channels,out_channels, strides, name,use_projection =True):\n",
    "    \n",
    "    # Only the first block per block_group uses projection shortcut and strides.\n",
    "    inputs = residual_unit(inputs, filter_size, in_channels, out_channels,strides ,use_projection = use_projection)\n",
    "    \n",
    "    for _ in range(1, blocks):\n",
    "        inputs = residual_unit(inputs, filter_size, in_channels, out_channels, 1, use_projection = False)\n",
    "    \n",
    "    return tf.identity(inputs, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "        \n",
    "    # get the shape of the input layer in a format [num_images, img_height, img_width, num_channels]\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # number of features is  img_height * img_width * num_channels\n",
    "    num_features = int(layer_shape[1] * layer_shape[2] * layer_shape[3])\n",
    "    \n",
    "    # reshape the layer to [num_images, num_features].\n",
    "    # -1  means the size in that dimension is calculated so the total size of the tensor is unchanged from the reshaping.\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features],name = \"FLAT_LAYER\")\n",
    "\n",
    "    # return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_fc_layer(input, num_inputs,num_outputs,use_relu, use_sigmoid): \n",
    "\n",
    "    # new weights and biases for the layer\n",
    "    weights = new_weights(shape = [num_inputs, num_outputs])\n",
    "    biases = new_biases(length = num_outputs)\n",
    "\n",
    "    # calculate the layer as the matrix multiplication of the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer, name = \"FULLY_CONNECTED_WITH_RELU\")\n",
    "       \n",
    "    if use_sigmoid:\n",
    "        layer = tf.nn.sigmoid(layer,name = \"FULLY_CONNECTED_WITH_SIGMOID\")\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Architecture of 34-layer residual neural network:\n",
    "*  **conv1** (7x7 conv, 64,/2) -> ***filter_size***=7, ***out_channels***=64, ***stride***=2\n",
    "*  **pooling layer** ***stride***=2\n",
    "*  **block1** layers 6x[(3x3 con,64,)] -> 6 conv layers with:    ***filter_size***=3, ***out_channels***=64, ***stride***=1\n",
    "*  **block2** layers 8x[(3x3 con,128,)] -> 8 conv layers with:   ***filter_size***=3, ***out_channels***=128, ***stride***=1\n",
    "*  **block3** layers 12x[(3x3 con,256,)] -> 12 conv layers with: ***filter_size***=3, ***out_channels***=265, ***stride***=1 \n",
    "*  **block4** layers 6x[(3x3 con,512,)] -> 6 conv layers with:   ***filter_size***=3, ***out_channels***=512, ***stride***=1\n",
    "* **average pooling**\n",
    "* **fully connected layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMETERS OF THE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_size_par = 3\n",
    "\n",
    "block_1_in_channels  = 64\n",
    "block_1_out_channels = 64\n",
    "num_blocks_1         = 3\n",
    "\n",
    "block_2_in_channels  = 64\n",
    "block_2_out_channels = 128\n",
    "num_blocks_2         = 4\n",
    "\n",
    "block_3_in_channels  = 128\n",
    "block_3_out_channels = 256\n",
    "num_blocks_3         = 6\n",
    "\n",
    "block_4_in_channels  = 256\n",
    "block_4_out_channels = 512\n",
    "num_blocks_4         = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT\n",
    "**Image** of shape `fingerprint_size` by `fingerprint_size` and `num_channels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = new_conv_layer(input = drug_image ,filter_size=7,in_channels =1,out_channels=64, stride=2)\n",
    "conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = batch_norm_and_RELU(conv1)\n",
    "conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_layer = new_pooling_layer(input= conv1, dim_ksize = 3,stride = 2)\n",
    "pooling_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOCK 1\n",
    "layers 6x[(3x3 con,64,)] -> 6 conv layers with:    ***filter_size***=3, ***out_channels***=64, \n",
    "<br>\n",
    "or 3 times residual unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block1 = block_group(inputs = pooling_layer, blocks = num_blocks_1,filter_size = filter_size_par ,in_channels = block_1_in_channels,\n",
    "                     out_channels = block_1_out_channels , strides = 1, name= \"BLOCK1\",use_projection=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOCK 2\n",
    "layers 8x[(3x3 con,128,)] -> 8 conv layers with:    ***filter_size***=3, ***out_channels***=128, \n",
    "<br>\n",
    "or 4 times residual unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block2 = block_group(inputs = block1, blocks = num_blocks_2,filter_size = filter_size_par ,in_channels = block_2_in_channels,\n",
    "                     out_channels = block_2_out_channels , strides = 2, name= \"BLOCK2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOCK 3\n",
    "layers 12 x[(3x3 con,256,)] -> 12 conv layers with:    ***filter_size***=3, ***out_channels***=256, \n",
    "<br>\n",
    "or 6 times residual unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block3 = block_group(inputs = block2, blocks = num_blocks_3,filter_size = filter_size_par ,in_channels = block_3_in_channels,\n",
    "                     out_channels = block_3_out_channels , strides = 2, name= \"BLOCK3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOCK 4 \n",
    "layers 6x[(3x3 con,512,)] -> 6 conv layers with:    ***filter_size***=3, ***out_channels***=512, \n",
    "<br>\n",
    "or  times residual unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block4 = block_group(inputs = block3, blocks = num_blocks_4,filter_size = filter_size_par ,in_channels = block_4_in_channels,\n",
    "                     out_channels = block_4_out_channels , strides = 2, name= \"BLOCK3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AVE POOLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ave_pooling = tf.layers.average_pooling2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_flat, features_num = flatten_layer(output_ave_pooling )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer1 = new_fc_layer(input = layer_flat,\n",
    "                         num_inputs = features_num,\n",
    "                         num_outputs = fc_size1,\n",
    "                         use_relu = False,\n",
    "                         use_sigmoid = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.round(fc_layer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function to Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits = fc_layer1,\n",
    "                                                        labels = y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiply logistic loss with weights (ELEMENT-WISE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of cost for all labels with weight 1\n",
    "cost_sum = tf.reduce_sum(tf.multiply(cross_entropy_weights,cross_entropy))\n",
    "\n",
    "# number of labels with weight 1\n",
    "num_nonzero_weights = tf.count_nonzero(input_tensor=cross_entropy_weights,dtype = tf.float32)\n",
    "\n",
    "# average cost\n",
    "cost = tf.divide(cost_sum, num_nonzero_weights, name= \"COST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "accuracy, accuracy_ops =tf.metrics.accuracy(labels=y_true,predictions=output, weights = cross_entropy_weights)\n",
    "# Local variables need to show updated accuracy on each iteration \n",
    "stream_vars = [i for i in tf.local_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create TensorFlow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "session.run(init)\n",
    "saver = tf.train.Saver()\n",
    "train_batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_batch(batch_size, available_indexes):\n",
    "    chosen = np.random.choice(available_indexes,batch_size, replace=False)\n",
    "    available_indexes = set(available_indexes) - set(chosen)\n",
    "    X_batch = drug_fingerprints[chosen, :]\n",
    "    y_batch = drug_targets[chosen, :]\n",
    "    cross_entropy_weights = drug_weights[chosen,:]\n",
    "    return X_batch,y_batch,cross_entropy_weights, list(available_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter for total number of epochs\n",
    "total_epochs = 0\n",
    "\n",
    "def optimize(num_epochs):\n",
    "    \n",
    "    # update the global variable rather than a local copy.\n",
    "    global total_epochs\n",
    "\n",
    "    # start-time \n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(total_epochs, total_epochs + num_epochs):\n",
    "\n",
    "        for j in range(int(len(drug_targets)/train_batch_size)):\n",
    "            if j == 0:\n",
    "                available_indexes = list(range(len(drug_targets)))                         \n",
    "            x_batch,y_true_batch, weights_batch, available_indexes = fetch_batch(train_batch_size, available_indexes)\n",
    "\n",
    "            # put the batch into a dict with the proper names for placeholder variables\n",
    "            feed_dict_train = {x: x_batch,\n",
    "                               y_true: y_true_batch,\n",
    "                              cross_entropy_weights: weights_batch}\n",
    "\n",
    "            # run the optimizer with the btch training data\n",
    "            session.run(optimizer, feed_dict=feed_dict_train)\n",
    "            # save the model's weights at the end of each epoch\n",
    "            saver.save(session, \"./temp/my_model_ResNet.ckpt\")\n",
    "\n",
    "            # print update every 10 iterations\n",
    "            if j % 20 == 0:\n",
    "\n",
    "                # calculate the accuracy on the training-set.\n",
    "                acc_ops = session.run(accuracy_ops, feed_dict=feed_dict_train)\n",
    "\n",
    "                # print update\n",
    "                print('[Total correct, Total count]:',session.run(stream_vars)) \n",
    "                print(\"Epoch: {}, Optimization Iteration (batch #): {}, Training Accuracy: {} \\n\".format(i+1,j+1,acc_ops))                        \n",
    "\n",
    "        # update the total number of iterations\n",
    "    total_epochs += num_epochs\n",
    "\n",
    "    # end time\n",
    "    end_time = time.time()\n",
    "\n",
    "    # difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    #time-usage\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize(num_epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path= saver.save(session, \"./temp/my_model_ResNet_final.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
