{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_fingerprints_fh = 'sample/sample_fingerprints.csv'\n",
    "drug_targets_fh      = 'sample/sample_targets.csv'\n",
    "drug_weights_fh      = 'sample/sample_weights.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size       = 10000\n",
    "fingerprint_size  = 1024\n",
    "fingerprint_width = 32\n",
    "targets_num       = 420\n",
    "weights_num       = 420\n",
    "num_channels      = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def populate_data(file_handle,data_matrix, data_size):\n",
    "    with open(file_handle) as fh:\n",
    "        j=0\n",
    "        content = fh.readlines()\n",
    "        content = [x.strip() for x in content]\n",
    "        for line in content:\n",
    "            result = re.split(r'[,\\t]\\s*',line)\n",
    "            for i in range(1,data_size+1):\n",
    "                data_matrix[j][i-1] = np.float32(result[i])\n",
    "            j = j+1\n",
    "    print(j)\n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_fingerprints = []\n",
    "drug_targets      = []\n",
    "drug_weights      = []\n",
    "\n",
    "\n",
    "for i in range(sample_size):\n",
    "    fingerprint_holder = [0]* fingerprint_size\n",
    "    drug_fingerprints.append(fingerprint_holder)\n",
    "    \n",
    "for i in range(sample_size):\n",
    "    target_holder = [0]* targets_num\n",
    "    drug_targets.append(target_holder)\n",
    "\n",
    "for i in range(sample_size):\n",
    "    weight_holder = [0]* weights_num\n",
    "    drug_weights.append(weight_holder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populate_data(drug_weights_fh, drug_weights, weights_num)\n",
    "populate_data(drug_targets_fh, drug_targets, targets_num)\n",
    "populate_data(drug_fingerprints_fh, drug_fingerprints, fingerprint_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_fingerprints = np.array(drug_fingerprints)\n",
    "drug_targets      = np.array(drug_targets)\n",
    "drug_weights      = np.array(drug_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/mjdietzx/0cb95922aac14d446a6530f87b3a04ce#file-residual_network-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tensorflow/tpu/blob/master/models/official/resnet/resnet_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **TensorFlow** graph consists of the following parts:\n",
    "\n",
    "* **Placeholder** variables used for inputting data to the graph.\n",
    "* **Variables** that are going to be optimized so as to make the convolutional network perform better.\n",
    "* The mathematical formulas for the convolutional network.\n",
    "* **Cost function** be used to guide the optimization of the variables.\n",
    "* **Optimization** method which updates the variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, fingerprint_size],name = \"In_Flat_Drug_Fingerprint\")\n",
    "\n",
    "drug_image = tf.reshape(x, [-1, fingerprint_width, fingerprint_width, num_channels], name=\"Drug_Image_32x32\")\n",
    "\n",
    "inputs = tf.transpose(drug_image, [0, 3, 1, 2])\n",
    "\n",
    "y_true = tf.placeholder(tf.float32, [None, targets_num],name='True_Labels')\n",
    "\n",
    "cross_entropy_weights = tf.placeholder(tf.float32, [None, weights_num],name = \"Cross_Entropy_Weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05), name=\"Weights\")\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]), name=\"Biases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NETWORK ARCHITECTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/ResNetPic.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Description\n",
    "*  **conv1** (7x7 conv, 64,/2) -> ***filter_size***=7, ***out_channels***=64, ***stride***=2\n",
    "*  **pooling layer** ***stride***=2\n",
    "*  **block1** layers 6x[(3x3 con,64,)] -> 6 conv layers with:    ***filter_size***=3, ***out_channels***=64, ***stride***=1\n",
    "*  **block2** layers 8x[(3x3 con,128,)] -> 8 conv layers with:   ***filter_size***=3, ***out_channels***=128, ***stride***=1\n",
    "*  **block3** layers 12x[(3x3 con,256,)] -> 12 conv layers with: ***filter_size***=3, ***out_channels***=265, ***stride***=1 \n",
    "*  **block4** layers 6x[(3x3 con,512,)] -> 6 conv layers with:   ***filter_size***=3, ***out_channels***=512, ***stride***=1\n",
    "* **average pooling**\n",
    "* **fully connected layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difficulty: Shortcut Connection (FIRST OROGINAL IMPLEMENTATION)\n",
    "The identity shortcuts can be directly used when the input and output are of the same dimensions (solid line shortcuts on the graph above). When the dimension increase, we have two options:\n",
    "<br>\n",
    "* (A) the shortcut still performs identity mapping with extra zero passed for increasing dimensions,\n",
    "* (B) the shortcut is used to match dimensions (done by 1x1 convolution)\n",
    "<br>\n",
    "<br>\n",
    "For both options, when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/better_residue.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/two_res_unit.png\" alt=\"Drawing2\" style=\"width:300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HELPERS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_padding(inputs, kernel_size, data_format):\n",
    "    \n",
    "    pad_total = kernel_size - 1\n",
    "    pad_beg = pad_total // 2\n",
    "    pad_end = pad_total - pad_beg\n",
    "    \n",
    "    if data_format == 'channels_first':\n",
    "        padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],[pad_beg, pad_end], [pad_beg, pad_end]])\n",
    "    else:\n",
    "        padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n",
    "                                    [pad_beg, pad_end], [0, 0]])\n",
    "    return padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_fixed_padding(inputs, filters, kernel_size, strides, data_format):\n",
    "    \n",
    "    if strides > 1:\n",
    "        inputs = fixed_padding(inputs, kernel_size, data_format)\n",
    "        \n",
    "    return tf.layers.conv2d(inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n",
    "                            padding=('SAME' if strides == 1 else 'VALID'), use_bias=False,\n",
    "                            kernel_initializer=tf.variance_scaling_initializer(), data_format=data_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_NORM_DECAY = 0.9\n",
    "BATCH_NORM_EPSILON = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm_relu(inputs, is_training, relu=True, init_zero=False, data_format='channels_first'):\n",
    "    if init_zero:\n",
    "        gamma_initializer = tf.zeros_initializer()\n",
    "    else:\n",
    "        gamma_initializer = tf.ones_initializer()\n",
    "        \n",
    "    axis = 3\n",
    "    inputs = tf.layers.batch_normalization(inputs=inputs, axis=axis, momentum=BATCH_NORM_DECAY,\n",
    "                                           epsilon=BATCH_NORM_EPSILON, center=True, scale=True, \n",
    "                                           training=is_training,fused=True, gamma_initializer=gamma_initializer)\n",
    "    if relu:\n",
    "        inputs = tf.nn.relu(inputs) \n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORIGINAL IMPLEMENTATION OF THE RESIDUAL UNIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_unit(inputs, filters, is_training, strides,\n",
    "                   use_projection=False, data_format='channels_first'):\n",
    "    shortcut = inputs\n",
    "    print(\"Residue unit:\")\n",
    "    if use_projection:\n",
    "        shortcut = conv2d_fixed_padding(\n",
    "        inputs=inputs, filters=filters, kernel_size=1, strides=strides,\n",
    "        data_format=data_format)\n",
    "        print(\"FIRST IN THE BLOCK\")\n",
    "    shortcut = batch_norm_relu(shortcut, is_training, relu=False,\n",
    "                               data_format=data_format)\n",
    "    print(\"Shortcut:\")\n",
    "    print(shortcut)\n",
    "    print(\"CONV 1\")\n",
    "    inputs = conv2d_fixed_padding(\n",
    "      inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n",
    "      data_format=data_format)\n",
    "\n",
    "    print(inputs)\n",
    "    print(\"\\n\")\n",
    "    inputs = batch_norm_relu(inputs, is_training, data_format=data_format)\n",
    "    print(\"CONV 2\")\n",
    "    \n",
    "    inputs = conv2d_fixed_padding(\n",
    "      inputs=inputs, filters=filters, kernel_size=3, strides=1,\n",
    "      data_format=data_format)\n",
    "    print(inputs)\n",
    "    print(\"\\n\") \n",
    "    inputs = batch_norm_relu(inputs, is_training, relu=False, init_zero=True,\n",
    "                           data_format=data_format)\n",
    "    \n",
    "    print(\"Added Shortcut: \")\n",
    "    sum_with_shortcut = inputs + shortcut\n",
    "    print(sum_with_shortcut)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return tf.nn.relu( sum_with_shortcut )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_group(inputs, filters, blocks, strides, is_training, name,data_format='channels_first'):\n",
    "    \n",
    "    inputs = residual_unit(inputs, filters, is_training, strides,\n",
    "                    use_projection=True, data_format=data_format)\n",
    "    \n",
    "    for _ in range(1, blocks):\n",
    "        inputs = residual_unit(inputs, filters, is_training, 1,\n",
    "                      data_format=data_format)\n",
    "\n",
    "    return tf.identity(inputs, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_fc_layer(input, num_inputs,num_outputs): \n",
    "\n",
    "    # new weights and biases for the layer\n",
    "    weights = new_weights(shape = [num_inputs, num_outputs])\n",
    "    biases = new_biases(length = num_outputs)\n",
    "\n",
    "    # calculate the layer as the matrix multiplication of the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "    \n",
    "    layer = tf.nn.sigmoid(layer,name = \"FULLY_CONNECTED_WITH_SIGMOID\")\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Architecture of 34-layer residual neural network:\n",
    "*  **conv1** (7x7 conv, 64,/2) -> ***filter_size***=7, ***out_channels***=64, ***stride***=2\n",
    "*  **pooling layer** ***stride***=2\n",
    "*  **block1** layers 6x[(3x3 con,64,)] -> 6 conv layers with:    ***filter_size***=3, ***out_channels***=64, ***stride***=1\n",
    "*  **block2** layers 8x[(3x3 con,128,)] -> 8 conv layers with:   ***filter_size***=3, ***out_channels***=128, ***stride***=1\n",
    "*  **block3** layers 12x[(3x3 con,256,)] -> 12 conv layers with: ***filter_size***=3, ***out_channels***=265, ***stride***=1 \n",
    "*  **block4** layers 6x[(3x3 con,512,)] -> 6 conv layers with:   ***filter_size***=3, ***out_channels***=512, ***stride***=1\n",
    "* **average pooling**\n",
    "* **fully connected layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMETERS OF THE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training          = True\n",
    "data_format          = 'channels_first'\n",
    "filter_size_par      = 3\n",
    "num_classes          = 420\n",
    "\n",
    "block_1_filters      = 64\n",
    "num_blocks_1         = 3\n",
    "\n",
    "block_2_filters      = 128\n",
    "num_blocks_2         = 4\n",
    "\n",
    "block_3_filters      = 256\n",
    "num_blocks_3         = 6\n",
    "\n",
    "block_4_filters      = 512\n",
    "num_blocks_4         = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT\n",
    "**Image** of shape `num_channels` by`fingerprint_size` by `fingerprint_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.transpose(drug_image, [0, 3, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = conv2d_fixed_padding(inputs=inputs, filters= 64 , kernel_size= 7,strides= 2 , data_format=data_format)\n",
    "conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = batch_norm_relu(conv1, is_training=is_training, data_format='channels_first')\n",
    "conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_layer = tf.layers.max_pooling2d(inputs=conv1, pool_size=3,strides=2, padding='SAME',data_format='channels_first')\n",
    "pooling_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOCK 1\n",
    "layers 6x[(3x3 con,64,)] -> 6 conv layers with:    ***filter_size***=3, ***out_channels***=64, \n",
    "<br>\n",
    "or 3 times residual unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block1= block_group(inputs=pooling_layer, filters=block_1_filters , blocks=num_blocks_1,strides=1, is_training=is_training,\n",
    "                      name='BLOCK_1',data_format=data_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOCK 2\n",
    "layers 8x[(3x3 con,128,)] -> 8 conv layers with:    ***filter_size***=3, ***out_channels***=128, \n",
    "<br>\n",
    "or 4 times residual unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block2 = block_group(inputs=block1, filters=block_2_filters , blocks=num_blocks_2, strides=2, is_training=is_training,\n",
    "                      name='BLOCK_2',data_format=data_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOCK 3\n",
    "layers 12 x[(3x3 con,256,)] -> 12 conv layers with:    ***filter_size***=3, ***out_channels***=256, \n",
    "<br>\n",
    "or 6 times residual unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block3 = block_group(inputs=block2, filters=block_3_filters , blocks=num_blocks_3, strides=2, is_training=is_training,\n",
    "                      name='BLOCK_3',data_format=data_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOCK 4 \n",
    "layers 6x[(3x3 con,512,)] -> 6 conv layers with:    ***filter_size***=3, ***out_channels***=512, \n",
    "<br>\n",
    "or  times residual unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block4 = block_group(inputs=block3, filters=block_4_filters , blocks=num_blocks_4, strides=2, is_training=is_training,\n",
    "                      name='BLOCK_4',data_format=data_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AVE POOLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_size = (1, 1)\n",
    "output_ave_pooling = tf.layers.average_pooling2d(\n",
    "    inputs=block4 , pool_size=pool_size, strides=1, padding='VALID',\n",
    "    data_format=data_format)\n",
    "output_ave_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_flat= tf.reshape(output_ave_pooling, [-1, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer1 = new_fc_layer(input = layer_flat, num_inputs = 512, num_outputs = num_classes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.round(fc_layer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function to Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits = fc_layer1,\n",
    "                                                        labels = y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiply logistic loss with weights (ELEMENT-WISE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of cost for all labels with weight 1\n",
    "cost_sum = tf.reduce_sum(tf.multiply(cross_entropy_weights,cross_entropy))\n",
    "\n",
    "# number of labels with weight 1\n",
    "num_nonzero_weights = tf.count_nonzero(input_tensor=cross_entropy_weights,dtype = tf.float32)\n",
    "\n",
    "# average cost\n",
    "cost = tf.divide(cost_sum, num_nonzero_weights, name= \"COST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "accuracy, accuracy_ops =tf.metrics.accuracy(labels=y_true,predictions=output, weights = cross_entropy_weights)\n",
    "# Local variables need to show updated accuracy on each iteration \n",
    "stream_vars = [i for i in tf.local_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create TensorFlow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "session.run(init)\n",
    "saver = tf.train.Saver()\n",
    "train_batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_batch(batch_size, available_indexes):\n",
    "    chosen = np.random.choice(available_indexes,batch_size, replace=False)\n",
    "    available_indexes = set(available_indexes) - set(chosen)\n",
    "    X_batch = drug_fingerprints[chosen, :]\n",
    "    y_batch = drug_targets[chosen, :]\n",
    "    cross_entropy_weights = drug_weights[chosen,:]\n",
    "    return X_batch,y_batch,cross_entropy_weights, list(available_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter for total number of epochs\n",
    "total_epochs = 0\n",
    "\n",
    "def optimize(num_epochs):\n",
    "    \n",
    "    # update the global variable rather than a local copy.\n",
    "    global total_epochs\n",
    "\n",
    "    # start-time \n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(total_epochs, total_epochs + num_epochs):\n",
    "\n",
    "        for j in range(int(len(drug_targets)/train_batch_size)):\n",
    "            if j == 0:\n",
    "                available_indexes = list(range(len(drug_targets)))                         \n",
    "            x_batch,y_true_batch, weights_batch, available_indexes = fetch_batch(train_batch_size, available_indexes)\n",
    "\n",
    "            # put the batch into a dict with the proper names for placeholder variables\n",
    "            feed_dict_train = {x: x_batch,\n",
    "                               y_true: y_true_batch,\n",
    "                              cross_entropy_weights: weights_batch}\n",
    "\n",
    "            # run the optimizer with the btch training data\n",
    "            session.run(optimizer, feed_dict=feed_dict_train)\n",
    "            # save the model's weights at the end of each epoch\n",
    "            saver.save(session, \"./temp/my_model_ResNet.ckpt\")\n",
    "\n",
    "            # print update every 10 iterations\n",
    "            if j % 20 == 0:\n",
    "\n",
    "                # calculate the accuracy on the training-set.\n",
    "                acc_ops = session.run(accuracy_ops, feed_dict=feed_dict_train)\n",
    "\n",
    "                # print update\n",
    "                print('[Total correct, Total count]:',session.run(stream_vars)) \n",
    "                print(\"Epoch: {}, Optimization Iteration (batch #): {}, Training Accuracy: {} \\n\".format(i+1,j+1,acc_ops))                        \n",
    "\n",
    "        # update the total number of iterations\n",
    "    total_epochs += num_epochs\n",
    "\n",
    "    # end time\n",
    "    end_time = time.time()\n",
    "\n",
    "    # difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    #time-usage\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize(num_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"./logs/ResNet\", session.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! tensorboard --logdir=log+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path= saver.save(session, \"./temp/my_model_ResNet_final.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
